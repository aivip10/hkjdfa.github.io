WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.609
Let's look at a basic model of

00:00:02.609 --> 00:00:06.765
an artificial neural network where we have only a single hidden layer.

00:00:06.764 --> 00:00:09.599
The inputs are each connected to the neurons in

00:00:09.599 --> 00:00:12.990
the hidden layer and the neurons in the hidden layer are each

00:00:12.990 --> 00:00:15.949
connected to the neurons in the output layer where

00:00:15.949 --> 00:00:19.489
each neuron there represents a single output.

00:00:19.489 --> 00:00:23.644
We can look at it as a collection of mathematical functions.

00:00:23.644 --> 00:00:26.390
Each input is connected mathematically to

00:00:26.390 --> 00:00:29.740
a hidden layer of neurons through a set of weights we need

00:00:29.739 --> 00:00:36.509
to modify and each hidden layer neuron is connected to the output layer in a similar way.

00:00:36.509 --> 00:00:40.074
There is no limit to the number of inputs,

00:00:40.075 --> 00:00:44.010
number of hidden neurons in a layer and number of outputs,

00:00:44.009 --> 00:00:47.585
nor are there any correlations between those numbers.

00:00:47.585 --> 00:00:50.344
So, we can have n inputs,

00:00:50.344 --> 00:00:54.535
m hidden neurons and k outputs.

00:00:54.534 --> 00:00:57.414
In a closer even more simplistic look,

00:00:57.414 --> 00:01:00.240
we can see that each input is multiplied by

00:01:00.240 --> 00:01:06.120
its corresponding weight and added at the next layers neuron with a bias as well.

00:01:06.120 --> 00:01:09.510
The bias is an external parameter of the neuron and can

00:01:09.510 --> 00:01:13.140
be modeled by adding an external fixed value input.

00:01:13.140 --> 00:01:15.829
This entire summation will usually go through

00:01:15.829 --> 00:01:20.090
an activation function to the next layer or to the output.

00:01:20.090 --> 00:01:22.344
But what is our goal?

00:01:22.344 --> 00:01:28.135
We can look at our system as a black box that has n inputs and k outputs.

00:01:28.135 --> 00:01:34.200
Our goal is to design the system in such a way that it will give us the correct output y,

00:01:34.200 --> 00:01:36.665
for a specific input x.

00:01:36.665 --> 00:01:40.865
Our job is to decide what's inside this black box.

00:01:40.864 --> 00:01:45.699
We know that we will use artificial neural networks and need to train it to

00:01:45.700 --> 00:01:51.230
eventually have a system that will yield the correct output to a specific input.

00:01:51.230 --> 00:01:54.145
Well correct most of the time.

00:01:54.144 --> 00:02:00.140
Essentially, what we really want is to find the optimal set of weights connecting

00:02:00.140 --> 00:02:03.070
the input to the hidden layer and

00:02:03.069 --> 00:02:07.019
the optimal set of weights connecting the hidden layer to the output.

00:02:07.019 --> 00:02:12.960
We will never have a perfect estimation but we can try to be as close to it as we can.

