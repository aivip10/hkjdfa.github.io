WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:05.620
解决这一问题的最佳方式是更改激活函数 再介绍一种激活函数

00:00:05.620 --> 00:00:12.788
双曲正切函数 公式在下方

00:00:12.788 --> 00:00:15.859
(ex - e(-x))/(ex + e(-x))

00:00:15.859 --> 00:00:17.995
这个函数和 s 型函数相似

00:00:17.995 --> 00:00:20.920
但是因为范围在 -1 到 1 之间

00:00:20.920 --> 00:00:22.984
所以导数更大些

00:00:22.984 --> 00:00:24.640
不管你信不信 这种小小的差别实际上

00:00:24.640 --> 00:00:27.565
让神经网络有了很大的改进

00:00:27.565 --> 00:00:32.945
另一种非常热门的激活函数是修正线性单元 简称 ReLU

00:00:32.945 --> 00:00:36.020
这是一个非常简单的函数

00:00:36.020 --> 00:00:38.799
如果为正值

00:00:38.798 --> 00:00:40.894
则返回相同的值

00:00:40.895 --> 00:00:44.405
如果是负值 则返回 0

00:00:44.405 --> 00:00:48.560
可以看做 x 和 0 之间的最大值

00:00:48.560 --> 00:00:50.880
这个函数比 s 型函数更常用

00:00:50.880 --> 00:00:53.969
它可以显著改善训练效果

00:00:53.969 --> 00:00:59.535
但是不会太牺牲准确性 因为如果数字是正数 则导数是 1

00:00:59.533 --> 00:01:01.543
很神奇的是

00:01:01.545 --> 00:01:06.504
这个函数几乎不能区分情形 但却能够形成如此复杂的非线性解决方案

00:01:06.504 --> 00:01:08.459
现在有了更好的激活函数

00:01:08.459 --> 00:01:12.599
当我们将导数相乘以对任何特定权重获得导数时

00:01:12.599 --> 00:01:15.974
结果将是稍微大点的数字

00:01:15.974 --> 00:01:21.305
使导数稍微大些并让我们能够进行梯度下降

00:01:21.305 --> 00:01:25.500
我们将用它的函数图表来表示 ReLU 单元

00:01:25.500 --> 00:01:30.718
这是一个多层感知器示例 其中具有大量 ReLU 激活单元

00:01:30.718 --> 00:01:34.338
注意 最后一个单元是 s 型函数

00:01:34.340 --> 00:01:38.655
因为最终输出依然需要为 0 到 1 之间的概率

00:01:38.655 --> 00:01:40.968
但是 如果让最后一个单元为 ReLU

00:01:40.968 --> 00:01:45.203
实际上最终会获得预测值的回归模型

00:01:45.203 --> 00:01:49.000
这将在纳米学位课程的递归神经网络中有用

