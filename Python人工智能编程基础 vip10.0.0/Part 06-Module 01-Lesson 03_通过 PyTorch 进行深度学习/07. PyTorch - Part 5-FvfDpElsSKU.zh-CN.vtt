WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.924
你好 欢迎观看本视频

00:00:02.924 --> 00:00:05.714
我将讲解推断和验证

00:00:05.714 --> 00:00:10.769
并演示我的 Fashion-MNIST 网络解决方案

00:00:10.769 --> 00:00:13.740
推断和验证是两个相关术语

00:00:13.740 --> 00:00:16.170
推断是用训练过的网络

00:00:16.170 --> 00:00:18.000
进行预测

00:00:18.000 --> 00:00:20.355
它来源于统计学

00:00:20.355 --> 00:00:24.269
推断统计学是对数据进行一定的预测

00:00:24.269 --> 00:00:27.480
但是这些预测的效果实际上可能太好

00:00:27.480 --> 00:00:30.300
你的网络对训练集

00:00:30.300 --> 00:00:34.905
学习地太好 无法泛化到网络未见过的数据

00:00:34.905 --> 00:00:36.615
这叫做过拟合

00:00:36.615 --> 00:00:38.039
为了检验过拟合

00:00:38.039 --> 00:00:39.659
需要进行验证

00:00:39.659 --> 00:00:42.269
验证主要是对网络未见过的测试集

00:00:42.270 --> 00:00:45.690
运行网络

00:00:45.689 --> 00:00:49.709
网络没有进行过这个测试集的训练 这些数据是从训练集中扩展出来的

00:00:49.710 --> 00:00:51.359
这样

00:00:51.359 --> 00:00:57.375
你会发现网络对它未见过的数据集运行良好

00:00:57.375 --> 00:01:00.960
因此 如果训练损失非常低

00:01:00.960 --> 00:01:03.060
但验证损失非常高

00:01:03.060 --> 00:01:05.359
这意味着网络过拟合

00:01:05.359 --> 00:01:11.355
根据验证集预测图像 会非常难

00:01:11.355 --> 00:01:12.990
因此 为了防止过拟合

00:01:12.989 --> 00:01:15.119
我们进行正则化 例如 丢弃法

00:01:15.120 --> 00:01:17.325
我将在这个 notebook 给你演示怎么做

00:01:17.325 --> 00:01:21.659
首先 导入数据包

00:01:21.659 --> 00:01:23.864
然后加载数据

00:01:23.864 --> 00:01:28.844
现在 我将给你演示如何为 Fashion-MNIST 数据集建立网络

00:01:28.844 --> 00:01:32.549
实际上 我做的稍微复杂一点 不过这里还没有演示

00:01:32.549 --> 00:01:34.965
希望对你有所启发

00:01:34.965 --> 00:01:37.079
我想建立网络

00:01:37.079 --> 00:01:40.260
传入随机数量的隐藏层

00:01:40.260 --> 00:01:43.680
设置每个隐藏层中的单元数量

00:01:43.680 --> 00:01:45.480
我的做法是

00:01:45.480 --> 00:01:49.245
建立网络 例如 nn.Module

00:01:49.245 --> 00:01:51.435
在 init 函数中

00:01:51.435 --> 00:01:59.895
我希望能传入输入大小 输出大小 隐藏层

00:01:59.894 --> 00:02:01.859
我将采用丢弃法

00:02:01.859 --> 00:02:04.650
可以在这里设置丢弃概率

00:02:04.650 --> 00:02:10.784
然后 添加随机数量的隐藏层

00:02:10.784 --> 00:02:14.379
使用 nn.ModuleList 实现这一点

00:02:14.389 --> 00:02:17.159
一般来说 ModuleList 的功能就像一般的列表一样

00:02:17.159 --> 00:02:18.210
附加内容

00:02:18.210 --> 00:02:20.400
添加索引等

00:02:20.400 --> 00:02:24.150
但是它一般只能帮助内部 PyTorch

00:02:24.150 --> 00:02:28.295
跟踪并显示网络架构

00:02:28.294 --> 00:02:30.989
这里只创建一个列表

00:02:30.990 --> 00:02:35.129
传入第一层的线性运算

00:02:35.129 --> 00:02:39.120
输入大小 隐藏层

00:02:39.120 --> 00:02:44.909
相当于第一个隐藏层的输入层

00:02:44.909 --> 00:02:50.189
现在需要在隐藏层列表中添加层

00:02:50.189 --> 00:02:52.935
例如 nn.Linear

00:02:52.935 --> 00:02:58.080
但是需要找到传入 h1 和 h2 的方法

00:02:58.080 --> 00:03:00.450
h1 是某个层的输入

00:03:00.449 --> 00:03:02.339
h2 是该层的输出

00:03:02.340 --> 00:03:06.495
因此 从这里输入的隐藏层

00:03:06.495 --> 00:03:08.520
是这样的

00:03:08.520 --> 00:03:10.695
250

00:03:10.694 --> 00:03:13.409
或者 200 100 50 就像这样

00:03:13.409 --> 00:03:16.620
列表中有三个层

00:03:16.620 --> 00:03:17.969
三个隐藏层

00:03:17.969 --> 00:03:19.094
第一层是 200 个单元

00:03:19.094 --> 00:03:20.280
第二层是 100 个单元

00:03:20.280 --> 00:03:21.765
第三层是 50 个单元

00:03:21.764 --> 00:03:27.914
我要做的就是 self.Linear(200, 100)

00:03:27.914 --> 00:03:30.944
self.Linear(100, 50)

00:03:30.944 --> 00:03:34.469
self.Linear(50, output_size)

00:03:34.469 --> 00:03:36.509
但是我想以编程的方式实现

00:03:36.509 --> 00:03:38.009
不想将这些数字硬编码

00:03:38.009 --> 00:03:39.944
因为我想从这些隐藏层中获取这些数字

00:03:39.944 --> 00:03:43.905
可以取这些数字对 200 100 100 50

00:03:43.905 --> 00:03:46.919
要做的就是 压缩隐藏层

00:03:46.919 --> 00:03:50.834
取从第一个元素到最后一个元素的切片

00:03:50.835 --> 00:03:55.320
然后是从第二个元素到最后一个元素的隐藏层

00:03:55.319 --> 00:04:00.209
解压后首先出现的是 200 和 100

00:04:00.210 --> 00:04:06.555
所以 解压后的第一个元素是

00:04:06.555 --> 00:04:08.189
200 和 100

00:04:08.189 --> 00:04:12.180
解压后的第二个元素是 100 和 50

00:04:12.180 --> 00:04:17.850
所以 隐藏层实际是以元组为单位

00:04:17.850 --> 00:04:21.615
现在可以使用 self.hidden_layers.extend

00:04:21.615 --> 00:04:22.800
这只是一个列表

00:04:22.800 --> 00:04:25.425
可以扩展这个列表

00:04:25.425 --> 00:04:30.030
添加线性运算

00:04:30.029 --> 00:04:33.854
这样就设置了随机数量的隐藏层

00:04:33.855 --> 00:04:43.095
我们想让输出层 self.output 成为另一个线性层

00:04:43.095 --> 00:04:45.390
输出大小

00:04:45.389 --> 00:04:50.769
就是这样 我还想使用丢弃法

00:04:50.769 --> 00:04:54.444
所以可以在这里添加一个丢弃模块

00:04:54.444 --> 00:04:57.654
nn.Dropout

00:04:57.654 --> 00:05:00.804
然后设置丢弃的概率

00:05:00.805 --> 00:05:04.644
可以硬编码 但我使用 drop_p

00:05:04.644 --> 00:05:07.449
它是网络的一个关键字参数

00:05:07.449 --> 00:05:12.384
我们已经定义了网络架构

00:05:12.384 --> 00:05:17.149
接下来是前向传播遍历

00:05:17.149 --> 00:05:21.495
这个列表里有隐藏层

00:05:21.495 --> 00:05:24.240
self. x 可以通过它进行循环

00:05:24.240 --> 00:05:27.185
for each in self.hidden_layers,

00:05:27.185 --> 00:05:29.115
对每个隐藏层

00:05:29.115 --> 00:05:31.230
传入 x

00:05:31.230 --> 00:05:34.105
传入 x 之后 张量到达隐藏层

00:05:34.105 --> 00:05:36.460
经过这个 ReLU 激活函数

00:05:36.459 --> 00:05:42.014
在激活操作之后 执行丢弃法

00:05:42.014 --> 00:05:44.360
for each in self.hidden_layer

00:05:44.360 --> 00:05:47.634
让 x 经过对应的线性函数

00:05:47.634 --> 00:05:51.159
然后经过这个 ReLU 激活函数

00:05:51.160 --> 00:05:54.885
之后经过丢弃法 最后经过 self.output

00:05:54.884 --> 00:06:02.404
现在返回 log_softmax(x, dim=one)

00:06:02.404 --> 00:06:04.609
返回对数几率之前

00:06:04.610 --> 00:06:07.530
但是现在实际返回的是 log_softmax

00:06:07.529 --> 00:06:12.334
log_softmax 可以使用一个很好的条件损失函数

00:06:12.334 --> 00:06:15.919
记住 我说的是在高难度的计算中

00:06:15.920 --> 00:06:22.694
如何利用 softmax 获得这些浮点和准确度

00:06:22.694 --> 00:06:24.980
因为有些数字非常接近零

00:06:24.980 --> 00:06:27.455
有些非常接近一

00:06:27.454 --> 00:06:29.824
但是在对数空间内操作时

00:06:29.824 --> 00:06:32.939
数字结果好得多

00:06:32.939 --> 00:06:35.714
例如 负四等

00:06:35.714 --> 00:06:38.449
所以 在 softmax 概率分布的对数空间里

00:06:38.449 --> 00:06:42.534
计算容易得多

00:06:42.535 --> 00:06:45.100
因为有 log_softmax

00:06:45.100 --> 00:06:49.189
如果想从 softmax 中获得实际概率分布

00:06:49.189 --> 00:06:50.870
可以取指数

00:06:50.870 --> 00:06:55.283
好了 网络架构就是这样

00:06:55.283 --> 00:06:59.795
现在 需要建立网络 定义条件和优化器

00:06:59.795 --> 00:07:05.055
调用 model = network(784

00:07:05.055 --> 00:07:07.454
一般输入是 784 一般输出是10

00:07:07.454 --> 00:07:10.544
使用一个具有 500 个单元的隐藏层

00:07:10.545 --> 00:07:12.960
可以添加更多 200 500

00:07:12.959 --> 00:07:15.524
想添加多少都行

00:07:15.524 --> 00:07:19.739
将丢弃概率设为 0.5

00:07:19.740 --> 00:07:21.389
现在可以创建条件

00:07:21.389 --> 00:07:23.279
我之前讲过

00:07:23.279 --> 00:07:29.189
log_softmax 是前向传播遍历的输出

00:07:29.189 --> 00:07:34.410
所以这里可以使用 NLLLoss

00:07:34.410 --> 00:07:36.900
阅读这个文件就会发现

00:07:36.899 --> 00:07:39.569
这是负对数似然损失

00:07:39.569 --> 00:07:42.735
对分类有用

00:07:42.735 --> 00:07:45.080
通过正向调用获得的输入

00:07:45.079 --> 00:07:47.669
应包含每个类的对数概率

00:07:47.670 --> 00:07:49.324
就是这里写的内容

00:07:49.324 --> 00:07:51.620
所以 每个类都有对数概率

00:07:51.620 --> 00:07:54.100
这是我们需要使用的损失函数

00:07:54.100 --> 00:07:59.415
优化器 我们将使用 Adam 优化器

00:07:59.415 --> 00:08:06.319
Adam 是随机梯度下降的一个变形

00:08:06.319 --> 00:08:11.875
它通过动量增加训练速度 Ir=0.001

00:08:11.875 --> 00:08:14.600
将学习率设为 0.001

00:08:14.600 --> 00:08:17.270
现在已经全部设置好了

00:08:17.269 --> 00:08:20.044
可以开始正常编写训练代码

00:08:20.045 --> 00:08:22.520
训练两个周期

00:08:22.519 --> 00:08:24.543
步骤为 0

00:08:24.543 --> 00:08:27.484
运行损失设为零

00:08:27.485 --> 00:08:31.564
每 40 步打印一次损失

00:08:31.564 --> 00:08:35.725
for e in range (epochs)

00:08:35.725 --> 00:08:39.649
因为已经使用了丢弃法

00:08:39.649 --> 00:08:41.990
需要注意的是 在训练时

00:08:41.990 --> 00:08:43.190
我们希望启用丢弃法

00:08:43.190 --> 00:08:46.340
希望有一定的丢弃概率

00:08:46.340 --> 00:08:48.950
但是 在评估时

00:08:48.950 --> 00:08:52.520
尝试推断或验证时

00:08:52.519 --> 00:08:54.095
希望禁用丢弃法

00:08:54.095 --> 00:08:56.899
因为丢弃是随机

00:08:56.899 --> 00:09:00.034
关闭某些连接 丢弃某些单元

00:09:00.034 --> 00:09:02.059
换句话说

00:09:02.059 --> 00:09:04.474
在启用丢弃法的情况下 尝试执行推断时

00:09:04.475 --> 00:09:07.710
网络无法全容量运行

00:09:07.710 --> 00:09:09.230
所以 测量效果不好

00:09:09.230 --> 00:09:10.565
预测效果也不好

00:09:10.565 --> 00:09:14.930
所以希望能在执行推断时禁用丢弃法

00:09:14.929 --> 00:09:16.894
训练时再启用

00:09:16.894 --> 00:09:19.309
PyTorch 非常简单地实现了这一点

00:09:19.309 --> 00:09:20.899
执行 model.train 时

00:09:20.899 --> 00:09:22.264
启用丢弃法

00:09:22.264 --> 00:09:24.919
启用训练时需要的所有东西

00:09:24.919 --> 00:09:29.014
要想禁用丢弃法 输入 model.eval

00:09:29.014 --> 00:09:31.549
这样就将模型设置成

00:09:31.549 --> 00:09:34.745
推断和验证的评估模式

00:09:34.745 --> 00:09:36.830
我们想让这个循环

00:09:36.830 --> 00:09:38.975
从训练模式开始 输入 model.train

00:09:38.975 --> 00:09:41.029
训练时的完整遍历

00:09:41.029 --> 00:09:42.409
之前已经讲过

00:09:42.409 --> 00:09:44.269
不是新概念

00:09:44.269 --> 00:09:50.759
我们现在想打印训练损失

00:09:50.759 --> 00:09:56.090
print_every == 0

00:09:56.090 --> 00:09:57.355
打印训练损失时

00:09:57.355 --> 00:09:59.330
还想打印验证损失

00:09:59.330 --> 00:10:02.685
想运行网络

00:10:02.684 --> 00:10:05.819
通过网络运行测试集

00:10:05.820 --> 00:10:09.160
并测试它对之前所见数据的效果如何

00:10:09.159 --> 00:10:10.809
现在进行推断

00:10:10.809 --> 00:10:14.579
为了确保模型处于评估模式

00:10:14.580 --> 00:10:16.620
丢弃法会被禁用

00:10:16.620 --> 00:10:20.894
我们想知道准确度和测试损失

00:10:20.894 --> 00:10:25.039
准确度就是

00:10:25.039 --> 00:10:29.615
网络正确预测图像内容的概率

00:10:29.615 --> 00:10:36.049
我们将循环测试集中的图像和标签

00:10:36.049 --> 00:10:37.699
它是一个测试加载器

00:10:37.700 --> 00:10:40.850
图像缩放

00:10:40.850 --> 00:10:44.975
获得的图像和标签是变量

00:10:44.975 --> 00:10:47.870
对于变量

00:10:47.870 --> 00:10:51.379
设置 volatile=True

00:10:51.379 --> 00:10:57.779
意思就是 不想进行反向传播

00:10:57.779 --> 00:10:59.189
不想进行反向传播遍历

00:10:59.190 --> 00:11:02.100
这样的话 如果不关注运算过程

00:11:02.100 --> 00:11:05.415
就可以让 autograd 不再跟踪所有运算

00:11:05.414 --> 00:11:08.969
在进行推断和使用 PyTorch 模型时

00:11:08.970 --> 00:11:12.570
会这样设置 因为跟踪运算需要进行计算

00:11:12.570 --> 00:11:15.420
需要花费时间 所以 如果不保留这些梯度

00:11:15.419 --> 00:11:19.479
不跟踪当前进行的运算 速度就会加快

00:11:19.480 --> 00:11:21.095
所以 在推断模式下

00:11:21.095 --> 00:11:26.375
创建易失性为真的变量

00:11:26.375 --> 00:11:33.504
然后对图像进行前向传播遍历 获得 test_loss

00:11:33.504 --> 00:11:37.804
现在计算准确度

00:11:37.804 --> 00:11:40.859
要计算准确度 首先需要获得预测值

00:11:40.860 --> 00:11:44.754
根据 torch.exp 获得概率

00:11:44.754 --> 00:11:48.909
它会取指数 因为输出是 log_softmax

00:11:48.909 --> 00:11:50.370
如果取其指数

00:11:50.370 --> 00:11:54.629
会获得各个类之间的 softmax 概率分布

00:11:54.629 --> 00:11:59.009
获得概率 然后可以测试等同性

00:11:59.009 --> 00:12:01.830
我们想测试标签

00:12:01.830 --> 00:12:04.095
标签是真标签 对吧

00:12:04.095 --> 00:12:10.170
这些标签是否等于网络预测的标签？

00:12:10.169 --> 00:12:17.159
记住 这里的 ps 是概率分布 取最大值

00:12:17.159 --> 00:12:21.764
所以一般会找到概率最大的类

00:12:21.764 --> 00:12:23.879
给我们返回

00:12:23.879 --> 00:12:26.514
概率最大的类的索引

00:12:26.514 --> 00:12:30.730
这样对真标签和预测标签进行测试

00:12:30.730 --> 00:12:36.029
因为它预测的是概率最大的索引

00:12:36.029 --> 00:12:41.389
例如 网络提示“第九季”和提示“这是九”

00:12:41.389 --> 00:12:46.830
根据实际真标签与网络预测标签是否相同

00:12:46.830 --> 00:12:53.490
给我们返回“真”或者“假”

00:12:53.490 --> 00:12:57.634
然后可以计算准确度 等同性

00:12:57.634 --> 00:12:59.850
记住 准确度是

00:12:59.850 --> 00:13:03.509
网络所做的正确预测的次数

00:13:03.509 --> 00:13:07.674
除以它所做的预测总次数

00:13:07.674 --> 00:13:11.309
由于值是真和假

00:13:11.309 --> 00:13:15.844
实际可以转换成一和零

00:13:15.845 --> 00:13:17.930
真为一 假为零

00:13:17.929 --> 00:13:19.159
如果取均值

00:13:19.159 --> 00:13:21.929
会转换成正确预测总次数

00:13:21.929 --> 00:13:25.304
除以预测总次数

00:13:25.304 --> 00:13:27.044
如果取 equality.mean

00:13:27.044 --> 00:13:30.539
会获得网络的准确度

00:13:30.539 --> 00:13:34.230
如果尝试以这种方式运行代码 你会发现它会中断

00:13:34.230 --> 00:13:37.800
等同性

00:13:37.799 --> 00:13:39.134
类似于布尔数据类型

00:13:39.134 --> 00:13:41.309
类似布尔张量 而不是浮动张量

00:13:41.309 --> 00:13:45.604
但是 如果输入 .mean

00:13:45.605 --> 00:13:46.970
应该是浮动张量

00:13:46.970 --> 00:13:49.095
所以这里需要转换

00:13:49.095 --> 00:13:53.350
输入 type_as(torch.FloatTensor)

00:13:53.350 --> 00:13:56.490
从这里获得等同性

00:13:56.490 --> 00:13:57.810
它类似于布尔张量

00:13:57.809 --> 00:14:00.389
将其转换成

00:14:00.389 --> 00:14:03.689
浮动张量 取均值 就会获得准确度

00:14:03.690 --> 00:14:07.270
整个循环就是验证循环

00:14:07.269 --> 00:14:10.049
对测试加载器中的所有图像进行这种操作

00:14:10.049 --> 00:14:13.469
计算损失 计算准确度

00:14:13.470 --> 00:14:19.004
这样就可以知道 网络对它见过的数据的效果如何

00:14:19.004 --> 00:14:22.414
将其粘贴在这里

00:14:22.414 --> 00:14:26.355
打印当前周期

00:14:26.355 --> 00:14:27.960
训练损失

00:14:27.960 --> 00:14:29.009
测试损失

00:14:29.009 --> 00:14:30.819
以及测试准确度

00:14:30.820 --> 00:14:35.580
测试加载器给出了批数

00:14:35.580 --> 00:14:38.145
也就是循环的次数 对吧？

00:14:38.144 --> 00:14:39.480
获得了准确度

00:14:39.480 --> 00:14:41.639
将其相加 取均值

00:14:41.639 --> 00:14:45.240
除以循环总次数

00:14:45.240 --> 00:14:48.914
就是准确度平均值和测试损失平均值

00:14:48.914 --> 00:14:54.189
最后 需要将运行损失设为零

00:14:54.190 --> 00:14:57.860
我们还想将模型设为训练模式

00:14:57.860 --> 00:15:00.019
代码已编写完毕

00:15:00.019 --> 00:15:02.750
可以训练网络了

00:15:02.750 --> 00:15:06.200
训练两个周期之后

00:15:06.200 --> 00:15:10.720
获得的测试准确度约为 0.85 非常好

00:15:10.720 --> 00:15:14.274
现在可以添加网络 确保它能正常运行

00:15:14.274 --> 00:15:20.740
我不确定七是鞋子 但它认为七就是鞋子

00:15:20.740 --> 00:15:22.585
在下一部分

00:15:22.585 --> 00:15:25.000
我将演示如何保存已训练模型

00:15:25.000 --> 00:15:27.465
每次需要时都训练一个新网络

00:15:27.465 --> 00:15:30.110
这样做的意义不大

00:15:30.110 --> 00:15:32.909
所以应该训练并保存网络

00:15:32.909 --> 00:15:36.194
在需要进行推断或用新数据训练时

00:15:36.195 --> 00:15:38.580
重新加载 持续训练

00:15:38.580 --> 00:15:42.270
或用它进行推断 下一段视频见

