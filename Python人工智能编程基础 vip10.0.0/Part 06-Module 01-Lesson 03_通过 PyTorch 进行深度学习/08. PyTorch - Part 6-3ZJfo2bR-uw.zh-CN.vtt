WEBVTT
Kind: captions
Language: zh-CN

00:00:00.000 --> 00:00:02.040
欢迎回来 在这一段视频中

00:00:02.040 --> 00:00:07.395
我将演示如何保存和加载已通过 PyTorch 训练的模型

00:00:07.394 --> 00:00:11.489
再次声明 这一点非常重要 因为在大多数情况下

00:00:11.490 --> 00:00:15.750
需要用某些数据对网络进行训练 将其保存到磁盘

00:00:15.750 --> 00:00:18.780
以后可以加载 继续训练

00:00:18.780 --> 00:00:22.560
或用它进行推断 例如 预测

00:00:22.559 --> 00:00:27.074
我已经运行了大部分代码

00:00:27.074 --> 00:00:29.489
用 fashion hymnist 训练了网络

00:00:29.489 --> 00:00:35.924
可以看出 准确度大约是 84 - 85%

00:00:35.924 --> 00:00:38.219
现在已经训练了网络

00:00:38.219 --> 00:00:41.054
我们不想使用这些权重

00:00:41.054 --> 00:00:42.600
以前从没学习过这个问题

00:00:42.600 --> 00:00:47.385
想将其保存下来 以后可以重新加载 再次使用

00:00:47.384 --> 00:00:49.229
这些权重 这些参数

00:00:49.229 --> 00:00:53.069
实际都存储在 model.state_dict 中

00:00:53.070 --> 00:00:56.295
将其打印出来 现在可以看见网络

00:00:56.295 --> 00:00:58.440
有两个隐藏层

00:00:58.439 --> 00:01:01.530
这个输出是 500 这个输入是 500 输出是 100

00:01:01.530 --> 00:01:05.879
这个输出层有 10 个输出单元

00:01:05.879 --> 00:01:07.170
还使用了丢弃法

00:01:07.170 --> 00:01:08.820
网络就是这样

00:01:08.819 --> 00:01:11.759
state_dict 键

00:01:11.760 --> 00:01:15.740
显示隐藏层的权重为零 就是这里

00:01:15.739 --> 00:01:17.674
隐藏层有偏差

00:01:17.674 --> 00:01:19.640
第二个隐藏层有权重

00:01:19.640 --> 00:01:23.790
输出层也有权重和偏差

00:01:23.790 --> 00:01:26.640
权重和偏差张量

00:01:26.640 --> 00:01:30.209
实际存储在 model.state_dict 中

00:01:30.209 --> 00:01:32.789
这就是一般情况下需要保存的内容

00:01:32.790 --> 00:01:35.775
state_dict 的保存非常简单

00:01:35.775 --> 00:01:37.484
使用 torch.save

00:01:37.484 --> 00:01:42.629
然后传入 model.state_dict 命名为 checkpoint.pth

00:01:42.629 --> 00:01:45.390
好了 已经保存了

00:01:45.390 --> 00:01:48.390
保存之后 就可以在 state_dict 中加载

00:01:48.390 --> 00:01:52.245
state_dict = torch.load

00:01:52.245 --> 00:01:56.020
将 state_dict 同样命名为

00:01:56.420 --> 00:01:59.265
checkpoint.pth

00:01:59.265 --> 00:02:03.750
然后可以打印 .keys

00:02:03.750 --> 00:02:08.039
这样得出的层和我们在 layers.0.weight 中看到的相同

00:02:08.039 --> 00:02:09.479
都是一样的

00:02:09.479 --> 00:02:11.699
但是 state_dict 是自动加载的

00:02:11.699 --> 00:02:14.099
怎么将其加载到模型中？

00:02:14.099 --> 00:02:19.139
例如 model.load_state_dict 传入 state_dict

00:02:19.139 --> 00:02:22.579
请注意 模型应该已经存在

00:02:22.580 --> 00:02:24.235
应该已经创建

00:02:24.235 --> 00:02:28.470
创建时 权重和偏差是随机初始化的

00:02:28.469 --> 00:02:30.724
参数是随机初始化的

00:02:30.724 --> 00:02:33.030
刚才传入了 state_dict

00:02:33.030 --> 00:02:38.685
然后用我们之前训练过的参数替代这些随机参数

00:02:38.685 --> 00:02:40.545
看起来很简单

00:02:40.544 --> 00:02:43.849
保存 state_dict 可以将其重新加载到模型中

00:02:43.849 --> 00:02:46.659
但在实际情况中 不是一直这么简单

00:02:46.659 --> 00:02:48.549
我现在给你演示原因

00:02:48.550 --> 00:02:50.880
如果创建了一个新网络

00:02:50.879 --> 00:02:54.439
输入一般是 784 有 10 个输出单元

00:02:54.439 --> 00:03:01.409
但是现在我将使用三个隐藏层

00:03:01.409 --> 00:03:04.370
分别有 400 200 和 100 个单元

00:03:04.370 --> 00:03:09.215
好了 现在这个 state_dict 已经进行了预训练 对吧？

00:03:09.215 --> 00:03:11.960
已经创建了新模型

00:03:11.960 --> 00:03:16.820
我将尝试加载这个 state_dict 但它提示错误

00:03:16.819 --> 00:03:18.514
意思是说

00:03:18.514 --> 00:03:21.629
张量大小不一致 预期张量的大小

00:03:21.629 --> 00:03:23.924
是这样 但来源元素较多

00:03:23.925 --> 00:03:26.590
对吧？ 意思就是

00:03:26.590 --> 00:03:29.990
用这个检查点训练这个 state_dict 时

00:03:29.990 --> 00:03:33.219
使用的网络架构不同

00:03:33.219 --> 00:03:37.419
在原始网络中 第一层有 500 个单元

00:03:37.419 --> 00:03:39.699
现在第一层有 400 个单元

00:03:39.699 --> 00:03:44.754
问题就是 建立和训练网络

00:03:44.754 --> 00:03:46.629
以及重新加载 state_dict 时

00:03:46.629 --> 00:03:50.424
模型的架构必须完全相同

00:03:50.424 --> 00:03:53.990
如果在重新加载 state_dict 时

00:03:53.990 --> 00:03:58.140
代码可以生成具有不同架构的网络

00:03:58.139 --> 00:04:01.519
那么必须添加架构的信息

00:04:01.520 --> 00:04:04.610
例如 在这个网络中 由于

00:04:04.610 --> 00:04:07.430
隐藏层的数量是随机的

00:04:07.430 --> 00:04:13.010
需要在保存的检查点中添加隐藏层的信息

00:04:13.009 --> 00:04:15.560
要将模型重建成之前训练的架构

00:04:15.560 --> 00:04:20.680
需要保存 state_dict 和模型架构的所有信息

00:04:20.680 --> 00:04:25.230
可以创建一个字典

00:04:25.230 --> 00:04:28.410
将输入大小设为 784

00:04:28.410 --> 00:04:32.010
输出大小设为 10

00:04:32.009 --> 00:04:34.800
如果有一个模型

00:04:34.800 --> 00:04:40.960
可以运行 each.out_features for each in
model.hidden_layers

00:04:41.089 --> 00:04:47.250
请记住 model.hidden_layers 是这些线性运算的列表

00:04:47.250 --> 00:04:50.790
hidden_layers 是网络中层的列表

00:04:50.790 --> 00:04:54.090
为了获得实际值

00:04:54.089 --> 00:04:57.044
我已经根据单元数量设置了 hidden_layers

00:04:57.045 --> 00:04:59.475
在每个层上调用 out_features

00:04:59.475 --> 00:05:01.710
就是遍历这个隐藏层列表中的每个层

00:05:01.709 --> 00:05:03.884
然后获取这个层中的

00:05:03.884 --> 00:05:07.469
特征或单元数量

00:05:07.470 --> 00:05:10.020
然后可以保存 state_dict

00:05:10.019 --> 00:05:11.774
创建字典之后

00:05:11.774 --> 00:05:14.459
可以保存字典

00:05:14.459 --> 00:05:21.359
checkpoint, ‘checkpoint.pth’ 好了

00:05:21.360 --> 00:05:27.225
现在 检查点包含了重建模型所需的所有必要信息

00:05:27.225 --> 00:05:29.355
我想创建

00:05:29.355 --> 00:05:35.175
一个函数 叫做 load_checkpoint 给它指定一个文件路径

00:05:35.175 --> 00:05:38.280
然后可以加载检查点

00:05:38.279 --> 00:05:46.919
检查点是 torch.load(filepath) 然后就可以创建模型了

00:05:46.920 --> 00:05:48.975
我们的模型是网络

00:05:48.975 --> 00:05:52.580
从检查点获取这个模型的参数

00:05:52.579 --> 00:05:55.094
可以传入输入大小

00:05:55.095 --> 00:06:04.140
传入输出大小

00:06:04.139 --> 00:06:07.694
传入隐藏层

00:06:07.694 --> 00:06:10.855
这样就传入了

00:06:10.855 --> 00:06:15.085
创建模型所需的所有必要参数

00:06:15.084 --> 00:06:17.229
获得模型之后

00:06:17.230 --> 00:06:21.520
就可以加载 state_dict

00:06:21.519 --> 00:06:24.569
现在返回模型

00:06:24.569 --> 00:06:29.589
然后可以调用检查点 传入 checkpoint.pth

00:06:32.750 --> 00:06:36.129
我们加载了模型 并成功加载了

00:06:36.129 --> 00:06:39.644
state_dict 所有元素齐全

00:06:39.644 --> 00:06:44.859
这基本就是保存和加载网络的步骤

00:06:44.860 --> 00:06:47.949
还要记住

00:06:47.949 --> 00:06:51.439
用于建立网络的每个参数

00:06:51.439 --> 00:06:54.100
都必须包含在检查点中

00:06:54.100 --> 00:06:59.935
这样 重建的模型和之前保存的会完全相同

00:06:59.935 --> 00:07:02.329
好了 下一段视频见 谢谢

