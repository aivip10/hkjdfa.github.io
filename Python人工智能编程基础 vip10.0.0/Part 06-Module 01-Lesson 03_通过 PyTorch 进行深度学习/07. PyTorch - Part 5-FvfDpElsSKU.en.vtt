WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.924
Hello again. Welcome to this video,

00:00:02.924 --> 00:00:05.714
where I'm going to be talking about Inference and Validation,

00:00:05.714 --> 00:00:10.769
as well as showing you my solution for the Fashion-MNIST network.

00:00:10.769 --> 00:00:13.740
So, inference and validation are related terms.

00:00:13.740 --> 00:00:16.170
Inference is when you take your trained network,

00:00:16.170 --> 00:00:18.000
and then you use it to make predictions.

00:00:18.000 --> 00:00:20.355
This is borrowed from statistics;

00:00:20.355 --> 00:00:24.269
inferential statistics, where you have your data and you make some predictions about it.

00:00:24.269 --> 00:00:27.480
However, these predictions can actually be too good,

00:00:27.480 --> 00:00:30.300
and that your network learns

00:00:30.300 --> 00:00:34.905
your training set too well and it can't generalize to data it hasn't seen before.

00:00:34.905 --> 00:00:36.615
This is called overfitting.

00:00:36.615 --> 00:00:38.039
So, to test for overfitting,

00:00:38.039 --> 00:00:39.659
we use what's called validation.

00:00:39.659 --> 00:00:42.269
Basically, validation is you run your network

00:00:42.270 --> 00:00:45.690
on a test set that your network hasn't seen before.

00:00:45.689 --> 00:00:49.709
It hasn't been trained on it, it's just data that's held out from your training set.

00:00:49.710 --> 00:00:51.359
What this does is,

00:00:51.359 --> 00:00:57.375
you can see how well your network performs on a dataset that it hasn't seen before.

00:00:57.375 --> 00:01:00.960
So, if you get a really low training loss,

00:01:00.960 --> 00:01:03.060
but at a higher validation loss,

00:01:03.060 --> 00:01:05.359
that basically means your network is overfitting.

00:01:05.359 --> 00:01:11.355
It's having a hard time actually predicting images from your validation set.

00:01:11.355 --> 00:01:12.990
So, to prevent overfitting,

00:01:12.989 --> 00:01:15.119
we use regularization such as dropout.

00:01:15.120 --> 00:01:17.325
I'll show you how to do that in this notebook.

00:01:17.325 --> 00:01:21.659
To start off, I'm just going to import our packages,

00:01:21.659 --> 00:01:23.864
and then load our data.

00:01:23.864 --> 00:01:28.844
Now, I'll show you how I build my network for the Fashion-MNIST dataset.

00:01:28.844 --> 00:01:32.549
So, I actually did a little bit more complicated, and I haven't shown you yet.

00:01:32.549 --> 00:01:34.965
So, hopefully this will be instructive.

00:01:34.965 --> 00:01:37.079
So, what I want to do is build my network,

00:01:37.079 --> 00:01:40.260
so I can pass in an arbitrary number of the hidden layers.

00:01:40.260 --> 00:01:43.680
I could set the number of units in each of those hidden layers.

00:01:43.680 --> 00:01:45.480
So, what I did is,

00:01:45.480 --> 00:01:49.245
I just created my network, like normal-.module.

00:01:49.245 --> 00:01:51.435
So, then in the init function,

00:01:51.435 --> 00:01:59.895
I wanted to be able to pass in the input size and the output size, the hidden layers.

00:01:59.894 --> 00:02:01.859
I'm also going to be using dropout,

00:02:01.859 --> 00:02:04.650
so I can set a drop probability here.

00:02:04.650 --> 00:02:10.784
So, then I wanted to add an arbitrary number of hidden layers.

00:02:10.784 --> 00:02:14.379
To do this, I used nn.ModuleList.

00:02:14.389 --> 00:02:17.159
Basically, functions like NormalList,

00:02:17.159 --> 00:02:18.210
you can append to it,

00:02:18.210 --> 00:02:20.400
you can index it and so on.

00:02:20.400 --> 00:02:24.150
But basically, it just helpsï¼Œhelps Pytorch

00:02:24.150 --> 00:02:28.295
keep track of the architecture of your network and display it.

00:02:28.294 --> 00:02:30.989
So, here I'm just going to create a list,

00:02:30.990 --> 00:02:35.129
put in the linear operation for the first layer.

00:02:35.129 --> 00:02:39.120
So, input size, hidden layers.

00:02:39.120 --> 00:02:44.909
So, that's going to be like an input layer into the first of the hidden layers.

00:02:44.909 --> 00:02:50.189
So now, I need to add layers to my hidden layer list.

00:02:50.189 --> 00:02:52.935
So, it'll be like nn.Linear,

00:02:52.935 --> 00:02:58.080
but I need to have a way to put in h1 and h2,

00:02:58.080 --> 00:03:00.450
where h1 is the input to some layer,

00:03:00.449 --> 00:03:02.339
then the output of that layer.

00:03:02.340 --> 00:03:06.495
So, our hidden layers input from here,

00:03:06.495 --> 00:03:08.520
looks something like this.

00:03:08.520 --> 00:03:10.695
So, say like 250,

00:03:10.694 --> 00:03:13.409
or 200, 100, 50. Something like this.

00:03:13.409 --> 00:03:16.620
So, the list of like we have three layers,

00:03:16.620 --> 00:03:17.969
the first three hidden layers,

00:03:17.969 --> 00:03:19.094
the first one is 200 units,

00:03:19.094 --> 00:03:20.280
the second one is 100 units,

00:03:20.280 --> 00:03:21.765
the third one has 50 units.

00:03:21.764 --> 00:03:27.914
So, then what I need to do is something that looks like self.Linear 200 to 100,

00:03:27.914 --> 00:03:30.944
self.Linear 100 to 50,

00:03:30.944 --> 00:03:34.469
self.Linear 50 to output size.

00:03:34.469 --> 00:03:36.509
But I want to do this programmatically,

00:03:36.509 --> 00:03:38.009
I don't want to have to hard-coding these numbers,

00:03:38.009 --> 00:03:39.944
because I want to get them from this hidden layers.

00:03:39.944 --> 00:03:43.905
So, we can do it to get these pairs 200, 100, 100, and 50.

00:03:43.905 --> 00:03:46.919
What you have to do is zip hidden layers and then take

00:03:46.919 --> 00:03:50.834
a slice from the first element to the last,

00:03:50.835 --> 00:03:55.320
and then hidden layers from the second element to the last.

00:03:55.319 --> 00:04:00.209
So, the first thing that is going to come out of here is 200 and 100.

00:04:00.210 --> 00:04:06.555
So, the first element out of zip is going to be the first and then the second,

00:04:06.555 --> 00:04:08.189
so it's 200 and 100.

00:04:08.189 --> 00:04:12.180
Then the second element is going to come out are going to be 100 and 50.

00:04:12.180 --> 00:04:17.850
So, this will actually give us our hidden layer sizes as tuples.

00:04:17.850 --> 00:04:21.615
So, now I could use self.hidden layers.extend.

00:04:21.615 --> 00:04:22.800
This is just a list.

00:04:22.800 --> 00:04:25.425
So, we can use extend just like you would list,

00:04:25.425 --> 00:04:30.030
and then we're going to add our linear operations.

00:04:30.029 --> 00:04:33.854
So, that'll set up our arbitrary number of hidden layers.

00:04:33.855 --> 00:04:43.095
Therefore, our output layer self.output we want another linear layer here,

00:04:43.095 --> 00:04:45.390
and then output size.

00:04:45.389 --> 00:04:50.769
So they're up all there. I also want to use dropout,

00:04:50.769 --> 00:04:54.444
so I can add a dropout module here.

00:04:54.444 --> 00:04:57.654
This is simply nn.Dropout.

00:04:57.654 --> 00:05:00.804
Then you can set the probability for dropout.

00:05:00.805 --> 00:05:04.644
You can either hard-code it but I'm going to put it as drop p,

00:05:04.644 --> 00:05:07.449
which is one of our keyword arguments for our network.

00:05:07.449 --> 00:05:12.384
So, there we've defined the architecture of the network.

00:05:12.384 --> 00:05:17.149
Next up is our forward method.

00:05:17.149 --> 00:05:21.495
So, we have our hidden layers in this list,

00:05:21.495 --> 00:05:24.240
self dot hidden layers and so we can just loop through this.

00:05:24.240 --> 00:05:27.185
So, for each in self.hidden layers,

00:05:27.185 --> 00:05:29.115
then for each of those hidden layers,

00:05:29.115 --> 00:05:31.230
we're going to pass in x.

00:05:31.230 --> 00:05:34.105
So, you pass in x, the tensor goes to the hidden layer,

00:05:34.105 --> 00:05:36.460
goes through this relu activation function,

00:05:36.459 --> 00:05:42.014
and then we're going to put dropout on it after the action activation.

00:05:42.014 --> 00:05:44.360
So, for each layer, and hidden layers,

00:05:44.360 --> 00:05:47.634
we pass it through this corresponding linear function,

00:05:47.634 --> 00:05:51.159
and then it goes through this relu activation function,

00:05:51.160 --> 00:05:54.885
and then through dropout and then finally the last one, self.output.

00:05:54.884 --> 00:06:02.404
So, here I'm going to return the log softmax x dim one.

00:06:02.404 --> 00:06:04.609
So, before I was returning the logits,

00:06:04.610 --> 00:06:07.530
but now I'm actually just returning log softmax.

00:06:07.529 --> 00:06:12.334
So, there's a nice criterion loss function that we can use the log softmax,

00:06:12.334 --> 00:06:15.919
and remember I was talking about how with softmax,

00:06:15.920 --> 00:06:22.694
you can end up with these floating point and accuracies in really difficult computations,

00:06:22.694 --> 00:06:24.980
because some of your numbers are very close to zero,

00:06:24.980 --> 00:06:27.455
and some of them could be like really close to one.

00:06:27.454 --> 00:06:29.824
But when you work in the log-space,

00:06:29.824 --> 00:06:32.939
your numbers are much better behaved.

00:06:32.939 --> 00:06:35.714
So, you're like negative four or something like that.

00:06:35.714 --> 00:06:38.449
So, the computation is just much easier in

00:06:38.449 --> 00:06:42.534
the log-space of the softmax probability distribution.

00:06:42.535 --> 00:06:45.100
Then since with the log softmax,

00:06:45.100 --> 00:06:49.189
if we want to get out the actual probability distribution from softmax,

00:06:49.189 --> 00:06:50.870
then you just take the exponential.

00:06:50.870 --> 00:06:55.283
Right, and that is basically it for our network architecture.

00:06:55.283 --> 00:06:59.795
So, now we need to create the network and define our criterion and optimizer.

00:06:59.795 --> 00:07:05.055
So, we'll just call model network 784,

00:07:05.055 --> 00:07:07.454
input as normal, we want 10 out as normal.

00:07:07.454 --> 00:07:10.544
I'm just going to use a hidden layer with 500 units.

00:07:10.545 --> 00:07:12.960
So, we can add more, so 200, 500,

00:07:12.959 --> 00:07:15.524
whatever we want there,

00:07:15.524 --> 00:07:19.739
and set our drop probability to 0.5.

00:07:19.740 --> 00:07:21.389
Now, we can create our criterion.

00:07:21.389 --> 00:07:23.279
So, like I was talking about before,

00:07:23.279 --> 00:07:29.189
we have the log softmax as the output of our forward pass.

00:07:29.189 --> 00:07:34.410
So, we can use the NLLLoss with this.

00:07:34.410 --> 00:07:36.900
If you read the documentation here,

00:07:36.899 --> 00:07:39.569
so, it's the negative log likelihood loss,

00:07:39.569 --> 00:07:42.735
and it's useful for classification,

00:07:42.735 --> 00:07:45.080
and it's the input given through forward

00:07:45.079 --> 00:07:47.669
call is expected to contain the log probabilities of each class,

00:07:47.670 --> 00:07:49.324
which is what we have here.

00:07:49.324 --> 00:07:51.620
So, we have the log probabilities of each class.

00:07:51.620 --> 00:07:54.100
So, this is the loss function that we want to use.

00:07:54.100 --> 00:07:59.415
So, the optimizer, we are going to use the Adam Optimizer.

00:07:59.415 --> 00:08:06.319
So, Adam is a variation of stochastic gradient descent,

00:08:06.319 --> 00:08:11.875
but it uses momentum to increase the training speed, Ir.00.

00:08:11.875 --> 00:08:14.600
Set the learning rate to 0.001.

00:08:14.600 --> 00:08:17.270
Okay, so, now that is all set up,

00:08:17.269 --> 00:08:20.044
and now, we can start writing our training code like normal.

00:08:20.045 --> 00:08:22.520
So, train for two epochs,

00:08:22.519 --> 00:08:24.543
our steps with zero,

00:08:24.543 --> 00:08:27.484
set our running_loss zero here.

00:08:27.485 --> 00:08:31.564
We're going to print our losses every 40 steps,

00:08:31.564 --> 00:08:35.725
for e in range (epochs).

00:08:35.725 --> 00:08:39.649
So, now that we have this drop out,

00:08:39.649 --> 00:08:41.990
so one thing to note is that when we're training,

00:08:41.990 --> 00:08:43.190
we want drop out to be on.

00:08:43.190 --> 00:08:46.340
So we want there to be some probability for drop out.

00:08:46.340 --> 00:08:48.950
But when we are evaluating,

00:08:48.950 --> 00:08:52.520
when we're trying to do inference or validation,

00:08:52.519 --> 00:08:54.095
we need the drop out to be off.

00:08:54.095 --> 00:08:56.899
Because remember that drop out is just randomly

00:08:56.899 --> 00:09:00.034
shutting off some of the connections and like dropping out some of the units.

00:09:00.034 --> 00:09:02.059
So, what that means is that,

00:09:02.059 --> 00:09:04.474
drop out is on and you're trying to do inference,

00:09:04.475 --> 00:09:07.710
your network is not going to be working at its full capacity.

00:09:07.710 --> 00:09:09.230
So, you're not going to get a good measurement.

00:09:09.230 --> 00:09:10.565
You're going to get good predictions.

00:09:10.565 --> 00:09:14.930
So we want to be able to turn off drop out when we're doing inference.

00:09:14.929 --> 00:09:16.894
We want to be able to turn it on when we're training.

00:09:16.894 --> 00:09:19.309
So, PyTorch provides this pretty simply,

00:09:19.309 --> 00:09:20.899
if you do model.train,

00:09:20.899 --> 00:09:22.264
it turns on drop out,

00:09:22.264 --> 00:09:24.919
turns on everything that should be on while you're training,

00:09:24.919 --> 00:09:29.014
and then if you actually want to turn out stuff off, you say model.eval.

00:09:29.014 --> 00:09:31.549
So, you're basically setting up your model

00:09:31.549 --> 00:09:34.745
in evaluation mode for inference and validation.

00:09:34.745 --> 00:09:36.830
So, at the beginning of this loop, we want to start

00:09:36.830 --> 00:09:38.975
in train mode, so obviously, model.train.

00:09:38.975 --> 00:09:41.029
So just for the training pass,

00:09:41.029 --> 00:09:42.409
you've seen all this before,

00:09:42.409 --> 00:09:44.269
so it's not too interesting.

00:09:44.269 --> 00:09:50.759
Okay, so now, we're at the part where we want to print out our training loss,

00:09:50.759 --> 00:09:56.090
so print_every equals zero.

00:09:56.090 --> 00:09:57.355
So when we print out training loss,

00:09:57.355 --> 00:09:59.330
we also want to print out our validation loss.

00:09:59.330 --> 00:10:02.685
So we want to actually run our network,

00:10:02.684 --> 00:10:05.819
we want to run our test set through the network and

00:10:05.820 --> 00:10:09.160
test how well it's doing on data as we've seen before.

00:10:09.159 --> 00:10:10.809
So, we're doing inference here,

00:10:10.809 --> 00:10:14.579
so we want to make sure our model is in evaluation mode.

00:10:14.580 --> 00:10:16.620
This is going to shut off dropout.

00:10:16.620 --> 00:10:20.894
We want to know the accuracy and we also want to know the test_loss.

00:10:20.894 --> 00:10:25.039
So, accuracy is just what's the probability

00:10:25.039 --> 00:10:29.615
that our network actually predicts correctly what the image is.

00:10:29.615 --> 00:10:36.049
So, we're going to loop through the images and labels in our test set,

00:10:36.049 --> 00:10:37.699
so it's a test loader.

00:10:37.700 --> 00:10:40.850
Normal, image.resize.

00:10:40.850 --> 00:10:44.975
We're going to get our images and labels into variables.

00:10:44.975 --> 00:10:47.870
So here, what we want to do with our variable,

00:10:47.870 --> 00:10:51.379
we actually want to set volatile equals true.

00:10:51.379 --> 00:10:57.779
So basically, what this means is we don't actually want to do back propagation,

00:10:57.779 --> 00:10:59.189
we don't want to do a backward pass.

00:10:59.190 --> 00:11:02.100
So, what this does is it shuts off autograd

00:11:02.100 --> 00:11:05.415
from actually keeping track of all operations if you don't really care about that.

00:11:05.414 --> 00:11:08.969
So you want to do this when you're doing inference and

00:11:08.970 --> 00:11:12.570
with PyTorch models because like that takes a computation,

00:11:12.570 --> 00:11:15.420
it takes time, so it's just going to be faster if you just don't

00:11:15.419 --> 00:11:19.479
keep those gradients and we don't keep track of the operations that are happening.

00:11:19.480 --> 00:11:21.095
So, when your inference mode,

00:11:21.095 --> 00:11:26.375
you want to create your variables with volatile equals true.

00:11:26.375 --> 00:11:33.504
Then we will do a forward pass with our images and get our test_loss.

00:11:33.504 --> 00:11:37.804
Now, we want to calculate the accuracy.

00:11:37.804 --> 00:11:40.859
To calculate our accuracy, we need to get our predictions first.

00:11:40.860 --> 00:11:44.754
So we'll get our probabilities from torch.exp.

00:11:44.754 --> 00:11:48.909
So it's taking the exponential because remember that our output is the logs softmax.

00:11:48.909 --> 00:11:50.370
So if we take the exponential of that,

00:11:50.370 --> 00:11:54.629
we'll just get back our softmax probability distribution over our classes.

00:11:54.629 --> 00:11:59.009
So, this gets us our probabilities and then we can test for equality.

00:11:59.009 --> 00:12:01.830
So we want to know our labels.

00:12:01.830 --> 00:12:04.095
So labels is our true labels, right?

00:12:04.095 --> 00:12:10.170
So, are these equal to the labels that are predicted by our network?

00:12:10.169 --> 00:12:17.159
So, remember that ps here is our probability distribution and so we're taking the max.

00:12:17.159 --> 00:12:21.764
So basically, you're finding the class that has the highest probability.

00:12:21.764 --> 00:12:23.879
So, what one here does is it gives us back

00:12:23.879 --> 00:12:26.514
the index of the class with the highest probability.

00:12:26.514 --> 00:12:30.730
So that way, we are testing the true labels with the predicted labels.

00:12:30.730 --> 00:12:36.029
Because remember, this is predicting the index with the highest probability.

00:12:36.029 --> 00:12:41.389
So that's the when our network is saying like season nine and saying like that's a nine.

00:12:41.389 --> 00:12:46.830
So, this will basically give us true or false based on

00:12:46.830 --> 00:12:53.490
if the actual real labels are the same as the labels that were predicted by our network.

00:12:53.490 --> 00:12:57.634
Then, we can calculate our accuracy and equality.

00:12:57.634 --> 00:12:59.850
So remember, accuracy is basically

00:12:59.850 --> 00:13:03.509
just how many predictions

00:13:03.509 --> 00:13:07.674
that our network get right divided by the total number of predictions it made.

00:13:07.674 --> 00:13:11.309
So, since these are true and false,

00:13:11.309 --> 00:13:15.844
they actually convert to ones and zeros.

00:13:15.845 --> 00:13:17.930
So true is one, false is zero,

00:13:17.929 --> 00:13:19.159
so if you take the mean,

00:13:19.159 --> 00:13:21.929
it actually gets converted to a sum of

00:13:21.929 --> 00:13:25.304
all the correct ones divided by the total number of predictions you have.

00:13:25.304 --> 00:13:27.044
So if you just take equality.mean,

00:13:27.044 --> 00:13:30.539
then you will get back the accuracy of your network.

00:13:30.539 --> 00:13:34.230
So, if you actually just try to run the code like this, you'll find that it breaks,

00:13:34.230 --> 00:13:37.800
and basically, what happens is equality, I think,

00:13:37.799 --> 00:13:39.134
it's like a Boolean type,

00:13:39.134 --> 00:13:41.309
like a Boolean tensor instead of a float tensor,

00:13:41.309 --> 00:13:45.604
but when you say.mean,

00:13:45.605 --> 00:13:46.970
it needs to be a float tensor.

00:13:46.970 --> 00:13:49.095
So you actually have to convert it here.

00:13:49.095 --> 00:13:53.350
So say, type as torch.FloatTensor.

00:13:53.350 --> 00:13:56.490
So basically, we're taking equality which we get from here,

00:13:56.490 --> 00:13:57.810
it's like a Boolean tensor,

00:13:57.809 --> 00:14:00.389
we are converting it into

00:14:00.389 --> 00:14:03.689
a float tensor and then taking the mean and that gives us our accuracy.

00:14:03.690 --> 00:14:07.270
So this whole loop here is basically our validation loop.

00:14:07.269 --> 00:14:10.049
So we're going through all the images in the test loader and

00:14:10.049 --> 00:14:13.469
we are calculating the loss and we're calculating the accuracy.

00:14:13.470 --> 00:14:19.004
So, this will tell us how well our network is performing on data as it has seen.

00:14:19.004 --> 00:14:22.414
Okay, and then, just paste this in here.

00:14:22.414 --> 00:14:26.355
So basically, this is just like printing out what epoch we're on,

00:14:26.355 --> 00:14:27.960
what's our training loss,

00:14:27.960 --> 00:14:29.009
what's our test loss,

00:14:29.009 --> 00:14:30.819
and what's our test accuracy.

00:14:30.820 --> 00:14:35.580
So here, the test loader gets us the number of batches.

00:14:35.580 --> 00:14:38.145
So basically, the number of times you looped through here, right?

00:14:38.144 --> 00:14:39.480
So then, we have our accuracy,

00:14:39.480 --> 00:14:41.639
we're just adding it all up, and we're just taking the mean.

00:14:41.639 --> 00:14:45.240
So we're just dividing by the total number of loops here and here.

00:14:45.240 --> 00:14:48.914
So this is the average of our accuracy and the average of our test_loss.

00:14:48.914 --> 00:14:54.189
Finally, we need to set our running_loss back to zero at the end of here.

00:14:54.190 --> 00:14:57.860
We also want to set the model back to training.

00:14:57.860 --> 00:15:00.019
Now that we have it all written up,

00:15:00.019 --> 00:15:02.750
we can train our network.

00:15:02.750 --> 00:15:06.200
So now, after training for two epochs,

00:15:06.200 --> 00:15:10.720
got our test accuracy up to about 0.85. So, it's pretty good.

00:15:10.720 --> 00:15:14.274
Now, we can just add our network and make sure it actually works.

00:15:14.274 --> 00:15:20.740
So, I'm not exactly sure seven is the shoe but it thinks that seven is the shoe.

00:15:20.740 --> 00:15:22.585
So in the next part,

00:15:22.585 --> 00:15:25.000
I'm going to show you how to save your trained models.

00:15:25.000 --> 00:15:27.465
So, it doesn't really make a lot of sense to

00:15:27.465 --> 00:15:30.110
train a new network every time you need it, so typically,

00:15:30.110 --> 00:15:32.909
what you'll do is train your network and then save it and then when you

00:15:32.909 --> 00:15:36.194
need to do inference or maybe train it some more with new data,

00:15:36.195 --> 00:15:38.580
you'll load it back up and then keep training

00:15:38.580 --> 00:15:42.270
or use it for inference. See you next video.

